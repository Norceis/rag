{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:58:37.168155400Z",
     "start_time": "2023-10-22T19:58:31.324690200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cubix\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain.vectorstores import FAISS\n",
    "from dataset.parsing_data_utils import load_all_documents\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pinecone\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "raw_dataset_dir = Path(\"../../data/raw_dataset/html\")\n",
    "split_dataset_dir = Path('../../data/split_dataset/4')\n",
    "embedded_dataset_dir = Path('../../data/embedded_dataset/faiss/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RFCs: 9266file [00:27, 336.44file/s]\n"
     ]
    }
   ],
   "source": [
    "docs = load_all_documents(split_dataset_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:59:04.731221600Z",
     "start_time": "2023-10-22T19:58:37.170160700Z"
    }
   },
   "id": "c0381dc5af5e6f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding in progress:   0%|          | 130/607285 [00:22<28:49:16,  5.85it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = GPT4AllEmbeddings()\n",
    "global_db = FAISS.from_documents([docs[0]], embeddings)\n",
    "\n",
    "for idx, doc in tqdm(enumerate(docs[1:]), desc=\"Embedding in progress\", total=len(docs)):\n",
    "    global_db.add_documents([doc])\n",
    "    if not idx % 80000:\n",
    "        global_db.save_local(embedded_dataset_dir / f'faiss_idx_ckpt_{idx}')\n",
    "\n",
    "global_db.save_local(embedded_dataset_dir / f'faiss_idx')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-22T19:59:04.733221600Z"
    }
   },
   "id": "4cd821a5850f7612"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d1bc468d12884611"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a610f417f72d5f5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e7e93b0a201cec90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dd2c8b571b101b0e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "contents = [doc.page_content for doc in docs]\n",
    "metadatas = [doc.metadata for doc in docs]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:34:55.869938800Z",
     "start_time": "2023-10-22T19:34:55.624940200Z"
    }
   },
   "id": "7d3485deba78a48b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('notebooks/dataset/embedding_data.ipynb')}\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('module'), WindowsPath('/matplotlib_inline.backend_inline')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=118, Highest Compute Capability: 7.5.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Loading binary C:\\Users\\cubix\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cubix\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1099\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1099\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1030\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:986\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:680\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:850\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:228\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:42\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_outputs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     32\u001B[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001B[0;32m     33\u001B[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     40\u001B[0m     TokenClassifierOutput,\n\u001B[0;32m     41\u001B[0m )\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\modeling_utils.py:86\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_accelerate_available():\n\u001B[1;32m---> 86\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dispatch_model, infer_auto_device_map, init_empty_weights\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maccelerate\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     88\u001B[0m         check_tied_parameters_on_same_device,\n\u001B[0;32m     89\u001B[0m         find_tied_parameters,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     94\u001B[0m         set_module_tensor_to_device,\n\u001B[0;32m     95\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\accelerate\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.21.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maccelerator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Accelerator\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbig_modeling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      5\u001B[0m     cpu_offload,\n\u001B[0;32m      6\u001B[0m     cpu_offload_with_hook,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m     load_checkpoint_and_dispatch,\n\u001B[0;32m     12\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\accelerate\\accelerator.py:35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhooks\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhooks\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpointing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\accelerate\\checkpointing.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcuda\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mamp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GradScaler\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     25\u001B[0m     MODEL_NAME,\n\u001B[0;32m     26\u001B[0m     OPTIMIZER_NAME,\n\u001B[0;32m     27\u001B[0m     RNG_STATE_NAME,\n\u001B[0;32m     28\u001B[0m     SCALER_NAME,\n\u001B[0;32m     29\u001B[0m     SCHEDULER_NAME,\n\u001B[0;32m     30\u001B[0m     get_pretty_name,\n\u001B[0;32m     31\u001B[0m     is_tpu_available,\n\u001B[0;32m     32\u001B[0m     is_xpu_available,\n\u001B[0;32m     33\u001B[0m     save,\n\u001B[0;32m     34\u001B[0m )\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_tpu_available(check_device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\accelerate\\utils\\__init__.py:131\u001B[0m\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeepspeed\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    123\u001B[0m         DeepSpeedEngineWrapper,\n\u001B[0;32m    124\u001B[0m         DeepSpeedOptimizerWrapper,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    128\u001B[0m         HfDeepSpeedConfig,\n\u001B[0;32m    129\u001B[0m     )\n\u001B[1;32m--> 131\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbnb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfsdp_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\accelerate\\utils\\bnb.py:42\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_bnb_available():\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deepcopy\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\__init__.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the MIT license found in the\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cuda_setup, utils, research\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      8\u001B[0m     MatmulLtState,\n\u001B[0;32m      9\u001B[0m     bmm_cublas,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m     matmul_4bit\n\u001B[0;32m     14\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\research\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      3\u001B[0m     switchback_bnb,\n\u001B[0;32m      4\u001B[0m     matmul_fp8_global,\n\u001B[0;32m      5\u001B[0m     matmul_fp8_mixed,\n\u001B[0;32m      6\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py:8\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mbnb\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GlobalOptimManager\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OutlierTracer, find_outlier_dims\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# This source code is licensed under the MIT license found in the\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mbitsandbytes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m COMPILED_WITH_CUDA\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madagrad\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\bitsandbytes\\cextension.py:20\u001B[0m\n\u001B[0;32m     19\u001B[0m     CUDASetup\u001B[38;5;241m.\u001B[39mget_instance()\u001B[38;5;241m.\u001B[39mprint_log_stack()\n\u001B[1;32m---> 20\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'''\u001B[39m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001B[39m\n\u001B[0;32m     22\u001B[0m \n\u001B[0;32m     23\u001B[0m \u001B[38;5;124m    python -m bitsandbytes\u001B[39m\n\u001B[0;32m     24\u001B[0m \n\u001B[0;32m     25\u001B[0m \u001B[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001B[39m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001B[39m\u001B[38;5;124m'''\u001B[39m)\n\u001B[0;32m     28\u001B[0m lib\u001B[38;5;241m.\u001B[39mcadam32bit_grad_fp32 \u001B[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m embed_model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      3\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 5\u001B[0m embed_model \u001B[38;5;241m=\u001B[39m \u001B[43mHuggingFaceEmbeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membed_model_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencode_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m}\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\langchain\\embeddings\\huggingface.py:58\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import sentence_transformers python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     55\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install sentence_transformers`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     56\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m---> 58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m sentence_transformers\u001B[38;5;241m.\u001B[39mSentenceTransformer(\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name, cache_folder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_folder, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_kwargs\n\u001B[0;32m     60\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001B[0m, in \u001B[0;36mSentenceTransformer.__init__\u001B[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001B[0m\n\u001B[0;32m     87\u001B[0m         snapshot_download(model_name_or_path,\n\u001B[0;32m     88\u001B[0m                             cache_dir\u001B[38;5;241m=\u001B[39mcache_folder,\n\u001B[0;32m     89\u001B[0m                             library_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence-transformers\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     90\u001B[0m                             library_version\u001B[38;5;241m=\u001B[39m__version__,\n\u001B[0;32m     91\u001B[0m                             ignore_files\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mflax_model.msgpack\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrust_model.ot\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtf_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     92\u001B[0m                             use_auth_token\u001B[38;5;241m=\u001B[39muse_auth_token)\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodules.json\u001B[39m\u001B[38;5;124m'\u001B[39m)):    \u001B[38;5;66;03m#Load as SentenceTransformer model\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m     modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_sbert_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:   \u001B[38;5;66;03m#Load with AutoModel\u001B[39;00m\n\u001B[0;32m     97\u001B[0m     modules \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_auto_model(model_path)\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001B[0m, in \u001B[0;36mSentenceTransformer._load_sbert_model\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module_config \u001B[38;5;129;01min\u001B[39;00m modules_config:\n\u001B[0;32m    839\u001B[0m     module_class \u001B[38;5;241m=\u001B[39m import_from_string(module_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 840\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[43mmodule_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_config\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpath\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    841\u001B[0m     modules[module_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]] \u001B[38;5;241m=\u001B[39m module\n\u001B[0;32m    843\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m modules\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:137\u001B[0m, in \u001B[0;36mTransformer.load\u001B[1;34m(input_path)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(sbert_config_path) \u001B[38;5;28;01mas\u001B[39;00m fIn:\n\u001B[0;32m    136\u001B[0m     config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(fIn)\n\u001B[1;32m--> 137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Transformer(model_name_or_path\u001B[38;5;241m=\u001B[39minput_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:29\u001B[0m, in \u001B[0;36mTransformer.__init__\u001B[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case \u001B[38;5;241m=\u001B[39m do_lower_case\n\u001B[0;32m     28\u001B[0m config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_args, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir)\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(tokenizer_name_or_path \u001B[38;5;28;01mif\u001B[39;00m tokenizer_name_or_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m model_name_or_path, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokenizer_args)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m#No max_seq_length set. Try to infer from model\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:49\u001B[0m, in \u001B[0;36mTransformer._load_model\u001B[1;34m(self, model_name_or_path, config, cache_dir)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 49\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:492\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    489\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    490\u001B[0m     )\n\u001B[0;32m    491\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m--> 492\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m \u001B[43m_get_model_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    494\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    495\u001B[0m     )\n\u001B[0;32m    496\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    497\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    498\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    499\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:376\u001B[0m, in \u001B[0;36m_get_model_class\u001B[1;34m(config, model_mapping)\u001B[0m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_model_class\u001B[39m(config, model_mapping):\n\u001B[1;32m--> 376\u001B[0m     supported_models \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_mapping\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(supported_models, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    378\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m supported_models\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:666\u001B[0m, in \u001B[0;36m_LazyAutoMapping.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    664\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_type \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping:\n\u001B[0;32m    665\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping[model_type]\n\u001B[1;32m--> 666\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_attr_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m# Maybe there was several model types associated with this config.\u001B[39;00m\n\u001B[0;32m    669\u001B[0m model_types \u001B[38;5;241m=\u001B[39m [k \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_config_mapping\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;241m==\u001B[39m key\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:680\u001B[0m, in \u001B[0;36m_LazyAutoMapping._load_attr_from_module\u001B[1;34m(self, model_type, attr)\u001B[0m\n\u001B[0;32m    678\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules[module_name] \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers.models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgetattribute_from_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:625\u001B[0m, in \u001B[0;36mgetattribute_from_module\u001B[1;34m(module, attr)\u001B[0m\n\u001B[0;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attr, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    624\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(getattribute_from_module(module, a) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m attr)\n\u001B[1;32m--> 625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, attr)\n\u001B[0;32m    627\u001B[0m \u001B[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001B[39;00m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;66;03m# object at the top level.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1089\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1087\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[0;32m   1088\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1089\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1090\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1101\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1099\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1102\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1104\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:37:48.291317Z",
     "start_time": "2023-10-22T19:37:42.282805Z"
    }
   },
   "id": "c33beeebb03f2a54"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),\n",
    "    environment=os.environ.get('PINECONE_ENV')\n",
    ")\n",
    "\n",
    "index_name = 'test'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=384,\n",
    "        metric='cosine'\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:07.123573300Z",
     "start_time": "2023-10-22T19:03:06.905573300Z"
    }
   },
   "id": "142bd8838062f1b6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'dimension': 384,\n 'index_fullness': 0.0,\n 'namespaces': {},\n 'total_vector_count': 0}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:03:07.558085900Z",
     "start_time": "2023-10-22T19:03:07.124574900Z"
    }
   },
   "id": "9213d17831b17a02"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18978 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embed_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m i_end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mlen\u001B[39m(contents), i\u001B[38;5;241m+\u001B[39mbatch_size)\n\u001B[0;32m      7\u001B[0m contents_batch \u001B[38;5;241m=\u001B[39m contents[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[1;32m----> 8\u001B[0m embeds \u001B[38;5;241m=\u001B[39m \u001B[43membed_model\u001B[49m\u001B[38;5;241m.\u001B[39membed_documents(contents_batch)\n\u001B[0;32m      9\u001B[0m metadatas_batch \u001B[38;5;241m=\u001B[39m metadatas[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# add to Pinecone\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# for embed, metadata in zip(embeds, metadatas_batch):\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#     # Add error handling here if needed\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#         # Handle the exception, e.g., print an error message\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m#         print(f\"Error occurred: {e}\")\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'embed_model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(contents), batch_size)):\n",
    "    i_end = min(len(contents), i+batch_size)\n",
    "    contents_batch = contents[i:i+batch_size]\n",
    "    embeds = embed_model.embed_documents(contents_batch)\n",
    "    metadatas_batch = metadatas[i:i+batch_size]\n",
    "    # add to Pinecone\n",
    "    # for embed, metadata in zip(embeds, metadatas_batch):\n",
    "    #     # Add error handling here if needed\n",
    "    #     try:\n",
    "    #         # Send a single vector and metadata\n",
    "    #         index.upsert(vectors=zip(embed, metadata))\n",
    "    #     except Exception as e:\n",
    "    #         # Handle the exception, e.g., print an error message\n",
    "    #         print(f\"Error occurred: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T19:35:05.781506300Z",
     "start_time": "2023-10-22T19:35:05.726540800Z"
    }
   },
   "id": "3c2505ef09c49edd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21106e7be3a5a795"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "52b5557849f25eae"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "655084a82e2e48ca9a78d09ad39023f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92cbb4bbcc944046ba52dd0b5440eda5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94c9982d814e47fe953569310bc7ba71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c5f8070806e4ff19784e1fc1d58f7df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33762a3520c542e9bc6ac9f16a575fc6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "413b8c9a63db4e0ba8eb8bbf9f237f83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e50c9771a9264431814c049ff3bee91e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7f89d18182b4c48af4f8a11bb00e6db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c2826a42a8c4e958584709ea8376a86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2a6f097d54d467c9bce739efd021359"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aca01a9614a449b597c1afc205a268bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11b841a1bc254ae8a936635d333615a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71cb07581be04c809f7832d0f0de5414"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c19f1727cba24db2958aace6b7a9342b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T18:41:52.308291400Z",
     "start_time": "2023-10-22T18:41:29.106918100Z"
    }
   },
   "id": "9fa8cbb3ea76a503"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membed_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\langchain\\embeddings\\huggingface.py:92\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.embed_documents\u001B[1;34m(self, texts)\u001B[0m\n\u001B[0;32m     90\u001B[0m     sentence_transformers\u001B[38;5;241m.\u001B[39mSentenceTransformer\u001B[38;5;241m.\u001B[39mstop_multi_process_pool(pool)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 92\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mencode(texts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_kwargs)\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m embeddings\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:156\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    155\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 156\u001B[0m length_sorted_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort([\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_text_length(sen) \u001B[38;5;28;01mfor\u001B[39;00m sen \u001B[38;5;129;01min\u001B[39;00m sentences])\n\u001B[0;32m    157\u001B[0m sentences_sorted \u001B[38;5;241m=\u001B[39m [sentences[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m length_sorted_idx]\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatches\u001B[39m\u001B[38;5;124m\"\u001B[39m, disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:156\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    155\u001B[0m all_embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 156\u001B[0m length_sorted_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort([\u001B[38;5;241m-\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_text_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43msen\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m sen \u001B[38;5;129;01min\u001B[39;00m sentences])\n\u001B[0;32m    157\u001B[0m sentences_sorted \u001B[38;5;241m=\u001B[39m [sentences[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m length_sorted_idx]\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatches\u001B[39m\u001B[38;5;124m\"\u001B[39m, disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:571\u001B[0m, in \u001B[0;36mSentenceTransformer._text_length\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text)\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 571\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m([\u001B[38;5;28mlen\u001B[39m(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m text])\n",
      "File \u001B[1;32m~\\PycharmProjects\\rag\\venv39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:571\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text)\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 571\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m([\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m text])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-22T18:47:12.369468900Z",
     "start_time": "2023-10-22T18:46:58.460859500Z"
    }
   },
   "id": "fb89c567af37e3e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a064beb57baf31d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
